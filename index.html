<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Cloud Architechture Fan">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://mariolu.xyz/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Cloud Architechture Fan">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mario Lu">
<meta property="article:tag" content="Full Stack">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://mariolu.xyz"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/13/hello-world/" class="article-date">
  <time datetime="2020-01-13T13:35:05.112Z" itemprop="datePublished">2020-01-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/13/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2020/01/13/hello-world/" data-id="ck5cgafua00004o0279umcjs7" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-如何通过命令行访问unix-socket文件" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/13/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BF%E9%97%AEunix-socket%E6%96%87%E4%BB%B6/" class="article-date">
  <time datetime="2020-01-13T13:29:36.000Z" itemprop="datePublished">2020-01-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/13/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BF%E9%97%AEunix-socket%E6%96%87%E4%BB%B6/">如何通过命令行访问unix socket文件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>socket为什么不能用传统命令访问？</strong></p>
<p>socket文件不能通过普通的文件读写命令操作（比如说echo “xxx” &gt;<br>socket.file）它。因为它是在网络上面工作的。只能通过socket读写函数去操作它。</p>
<p><strong>socat和ncat命令</strong></p>
<p>其实通过的linux命令socat和ncat可以去操作socket。</p>
<p>其中-U指定了该文件是Unix域socket文件类型，ncat实现了类似于cat命令的访问unix<br>socket。</p>
<p>ncat -U /tmp/tbsocket1</p>
<p>ncat也可以通过映射socket文件到监听的端口上。那么通过curl可以发送请求到该监听端口，实现写操作。</p>
<p># 映射tcp的8080流量到unix socket</p>
<p>ncat -vlk 8080 -c ‘ncat -U /tmp/tbsocket1’</p>
<p># 通过curl发起http请求访问</p>
<p>curl <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a></p>
<p>也可以使用功能更强大的socat来实现。</p>
<p># 映射8080/tcp 到unix socket</p>
<p>socat -d -d TCP-LISTEN:8080,fork UNIX:/tmp/tbsocket1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2020/01/13/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BF%E9%97%AEunix-socket%E6%96%87%E4%BB%B6/" data-id="ck5chlax500004o02fki203hd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-惊群效应" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/13/%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94/" class="article-date">
  <time datetime="2020-01-13T13:16:21.000Z" itemprop="datePublished">2020-01-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/13/%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94/">惊群效应</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>一、服务器网络模型和惊群</strong></p>
<p>传统的服务器使用“listen-accept-创建通信socket”完成客户端的一次请求服务。在高并发服务模型中，服务器创建很多进程-单线程（比如apache<br>mpm）或者n进程：m线程比例创建服务线程（比如nginx<br>event）。机器上运行着不等数量的服务进程或线程。这些进程监听着同一个socket。这个socket是和客户端通信的唯一地址。服务器父子进程或者多线程模型都accept该socket，有几率同时调用accept。当一个请求进来，accept同时唤醒等待socket的多个进程，但是只有一个进程能accept到新的socket，其他进程accept不到任何东西，只好继续回到accept流程。这就是惊群效应。如果使用的是select/epoll+accept，则把惊群提前到了select/epoll这一步，多个进程只有一个进程能acxept到连接，因为是非阻塞socket，其他进程返回EAGAIN。</p>
<p><strong>二、accept惊群的解决</strong></p>
<p>所有监听同一个socket的进程在内核中中都会被放在这个socket的wait<br>queue中。当一个tcp<br>socket有IO事件变化，都会产生一个wake_up_interruptible()。该系统调用会唤醒wait<br>queue的所有进程。所以修复linux内核的办法是只唤醒一个进程，比如说替换wake函数为wake_one_interruptoble()。</p>
<p><strong>2.1、改进版本accept+reuse port</strong></p>
<p>没有开启reuse选项的socket只有一个wait queue，假设在开启了socket<br>REUSE_PORT选项，内核中为每个进程分配了单独的accept wait queue，每次唤醒wait<br>queue只唤醒有请求的进程。协议栈将socket请求均匀分配给每个accept wait<br>queue。reuse部分解决了惊群问题，但是本身存在一些缺点或bug，比如REUSE实现是根据客户端ip端口实现哈希，对同一个客户请求哈希到同一个服务器进程，但是没有实现一致性哈希。在进程数量扩展新的进程，由于缺少一致性哈希，当listen<br>socket的数目发生变化（比如新服务上线、已存在服务终止）的时候，根据SO_REUSEPORT的路由算法，在客户端和服务端正在进行三次握手的阶段，最终的ACK可能不能正确送达到对应的socket，导致客户端连接发生Connection<br>Reset，所以有些请求会握手失败。</p>
<p><strong>三、select/epoll模型</strong></p>
<p>在一个高并发的服务器模型中，每秒accept的连接数很多。accept成为一个占用cpu很高的系统调用。考虑使用多进程来accept。select由于可扩展性能比如epoll，select遍历所有socket，select对每次操作都是要循环遍历所有的fd，所以在高并发场景下，select性能差。在高并发场景epoll使用场景更多。</p>
<p><strong>3.1、epoll的EPOLL_EXCLUSIVE选项</strong></p>
<p>liunx<br>4.5内核在epoll已经新增了EPOLL_EXCLUSIVE选项，在多个进程同时监听同一个socket，只有一个被唤醒。</p>
<p><strong>四、应用层解决</strong></p>
<p>同一时间只让一个进程accept/select/epoll一个监听端口。这是应用层解决惊群的办法，伪代码如下</p>
<p>semop(…); // lock</p>
<p>epoll_wait(…);</p>
<p>accept(…);</p>
<p>semop(…); // unlock</p>
<p>… // manage the request</p>
<p>多进程使用sysv实现semop，多线程则使用mutex。比如说mpm模式下的httpd，nginx都是这种实现办法。但是这种办法sysv是个固定的内存大小。比如在终端敲入ipcs。ipcs是机器共享固定大小空间。所以一旦有很多进程分配忘了手动释放，有内存泄漏风险。</p>
<p><strong>4.1、httpd</strong></p>
<p>for (;;) {</p>
<p><strong>accept_mutex_on ();</strong></p>
<p>for (;;) {</p>
<p>fd_set accept_fds;</p>
<p>…</p>
<p>rc = select (last_socket+1, &amp;accept_fds, NULL, NULL, NULL);</p>
<p>…</p>
<p>for (i = first_socket; i &lt;= last_socket; ++i) {</p>
<p>if (FD_ISSET (i, &amp;accept_fds)) {</p>
<p>new_connection = accept (i, NULL, NULL);</p>
<p>}</p>
<p><strong>accept_mutex_off ();</strong></p>
<p>process the new_connection;</p>
<p>}</p>
<p>}</p>
<p><strong>4.2、nginx</strong></p>
<p>void ngx_process_events_and_timers(ngx_cycle_t *cycle) { …</p>
<p><strong>if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {</strong></p>
<p><strong>return;</strong></p>
<p><strong>}</strong></p>
<p><strong>…</strong></p>
<p><strong>if (ngx_accept_mutex_held) {</strong></p>
<p><strong>flags |= NGX_POST_EVENTS;</strong></p>
<p><strong>}</strong></p>
<p>…</p>
<p>(void) ngx_process_events(cycle, timer, flags);</p>
<p>ngx_event_process_posted(cycle, &amp;ngx_posted_accept_events);</p>
<p><strong>if (ngx_accept_mutex_held) {</strong></p>
<p><strong>ngx_shmtx_unlock(&amp;ngx_accept_mutex);</strong></p>
<p><strong>}</strong></p>
<p>ngx_event_process_posted(cycle, &amp;ngx_posted_events);</p>
<p><strong>}</strong></p>
<p><strong>4.3、少量进程监听同一个socket</strong></p>
<p>当然在低负债的环境下，也可以分配少量的进程，即使有惊群，影响也是不大的。</p>
<p><strong>五、tornado/golang/其他</strong></p>
<p><strong>5.1、tornado</strong></p>
<p>tornado使用IOLoop模块，在Python3，IOLoop是个asyncio event循环。Python<br>2则使用了epoll (Linux) or kqueue (BSD and Mac OS X) 否则选用select()。所以python<br>tornado在面对惊群问题其实是没有解决的。所以就是系统不解决惊群问题丢给应用层解决，应用层不解决丢给用户解决。笔者在tornado模拟业务源站行为，曾经开启了几百个进程。模拟行为很纯粹，就是根据X-Flux头的指定的大小，返回给用户相应大小的2xx响应。该程序不涉及磁盘io，不涉及内存大量拷贝操作，本应是cpu运算型，但是发现客户端在压测tornado，并发度1w左右，却服务端cpu跑满，并且连接超时的概率竟然有百分之四五十。使用python分析程序发现epoll<br>wait函数占用了40%左右的cpu时间。很显然就是遇到了惊群响应。后面用golang重新实现了服务器，就没有了惊群。</p>
<p><strong>5.2、golang</strong></p>
<p>为啥golang就没有惊群响应呢？笔者查看了一个关键包netFD的accept实现。</p>
<p>func (fd *netFD) accept() (netfd *netFD, err error) {</p>
<p>／／在这里序列化accept，避免惊群效应</p>
<p><strong>if err := fd.readLock(); err != nil {</strong></p>
<p><strong>return nil, er</strong></p>
<p><strong>}</strong></p>
<p>defer fd.readUnlock()</p>
<p>……</p>
<p>for {</p>
<p>s, rsa, err = accept(fd.sysfd)</p>
<p>if err != nil {</p>
<p>if err == syscall.EAGAIN {</p>
<p>／／tcp还没三次握手成功，阻塞读直到成功，同时调度控制权下放给gorontine</p>
<p>if err = fd.pd.WaitRead(); err == nil {</p>
<p>continue</p>
<p>}</p>
<p>} else if err == syscall.ECONNABORTED {</p>
<p>／／被对端关闭</p>
<p>continue</p>
<p>}</p>
<p>}</p>
<p>break</p>
<p>}</p>
<p>netfd, err = newFD(s, fd.family, fd.sotype, fd.net)</p>
<p>……</p>
<p>//fd添加到epoll队列中</p>
<p>err = netfd.init()</p>
<p>……</p>
<p>lsa, _ := syscall.Getsockname(netfd.sysfd)</p>
<p>netfd.setAddr(netfd.addrFunc()(lsa), netfd.addrFunc()(rsa))</p>
<p>return netfd, nil</p>
<p>}</p>
<p><strong>5.3 lighttpd及其他</strong></p>
<p>linghttpd使用场景建议只有一个worker，多个worker会有些功能兼容上的问题，所以lighttpd官方其实也没有解决惊群问题。其他服务器tomcat、nodejs等等因为其实在高并发上会搭配apache或者nginx协同使用，所以研究意义不大。</p>
<p><strong>六、总结</strong></p>
<p>管中窥豹、惊群问题说大不大，但是如果碰到，可能是限制高并发性能的重要一个瓶颈，在探索惊群问题解决上，对各个服务器模型的分析以及内核层调研中整理了这些想法，希望对大家有所帮助。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2020/01/13/%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94/" data-id="ck5ch3v1f0000xl02028b2kt8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-微型分布式架构设计范例" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/06/%E5%BE%AE%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%8C%83%E4%BE%8B/" class="article-date">
  <time datetime="2020-01-06T04:21:48.000Z" itemprop="datePublished">2020-01-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/%E5%BE%AE%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%8C%83%E4%BE%8B/">微型分布式架构设计范例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>序言（初衷）</strong></p>
<p>设计该系统初衷是基于描绘业务（或机器集群）存储模型，分析代理缓存服务器磁盘存储与回源率的关系。系统意义是在腾讯云成本优化过程中，量化指导机房设备扩容。前半部分是介绍背景，对CDN缓存模型做一些理论思考。后半部分会实际操作搭建一个微型但是五脏俱全的分布式通用系统架构，最后赋予该系统一些跟背景相关的功能，解决成本优化中遇到的实际问题。</p>
<p><strong>缓存服务器存储模型架构（背景）：</strong></p>
<p><img src="http://res.cloudinary.com/lumanyu/image/upload/v1578283471/mini_arch_system/543e98f71930064bef971c2d6784544d_pvhpe7.jpg" alt=""></p>
<p>图1 存储模型</p>
<p>腾讯CDN的线上路由是用户à分布于各地区各运营商的OC-&gt;SOC-&gt;SMid-&gt;源站。各个层级节点部署的都是缓存服务器。来自用户的部分请求流量命中服务器，另一部分产生回源流量。</p>
<p>随着业务带宽自然增长，用户端带宽增长，假设业务回源率不变的情况下，磁盘缓存淘汰更新（淘汰）速率变快，表现为以下业务瓶颈（iowait变高、回源带宽变高，由于磁盘空间大小受限的缓存淘汰导致回源率变高）。</p>
<p>为了说明这个原理。我们假设两个极端：一个是设备磁盘容量无限大，业务过来的流量缓存只受源站缓存规则受限。只要缓存没过期，磁盘可以无限缓存，回源流量只需要首次访问的流量，所以这个回源量（率）只跟业务特性（重复率）有关系。另一个极端是磁盘极限小（归零），那么无论业务设置缓存是否过期，客户端访问量都是1比1的回源量。假设业务平均的缓存周期是1个小时。那么这1个小时的首次缓存带宽（同一cache<br>key的多次访问，我们认为是一次）将是这个硬盘的所需要的空间。这个大小是合理的，可以保证磁盘足够容纳业务的量。假设这个量达不到，或者本来达到了，但是由于业务自然增长了，1个小时内地首次缓存带宽变多，硬盘空间也不够用。</p>
<p>设备扩容是个解决办法。但是压测系统在这之前，没有客观数据证明需要扩容多大设备。或者扩容多少设备没有进行灰度验证，设备到位拍脑袋直接线上部署机器。我们在实验机器进行线上日志的重放，模拟出存储模拟曲线，来指导线上机房合理的设备存储。这就是建设重放日志系统的意义。</p>
<p><strong>麻雀虽小，五脏俱全的重放日志模型（总览）</strong></p>
<p>这一章，我们定义了下列模块：</p>
<p>模拟日志服务器：下载线上某个机房的一段时间周期的访问日志。一个日志存放10分钟访问记录。机房有几台机器就下载几份日志。日志服务器同时提供任务分片信息的查询服务。假设我们需要重放任务id为pig_120t的任务切片。下图既为任务切片详情。</p>
<p><img src="http://res.cloudinary.com/lumanyu/image/upload/v1578283471/mini_arch_system/dd857b4d1d1ad9bb89542041090aefff_acd61h.jpg" alt=""></p>
<p>图2 日志服务器的日志分片文件</p>
<p>任务控制器：启动任务或者结束任务总开关。任务分配均匀分配给具体的肉鸡和代理服务器。插入任务到Task<br>Pool中，收集服务端的实时总流量、回源流量、总请求次数和回源次数数据并插入到回源率结果数据表。</p>
<p>肉鸡：轮询Task<br>Pool的任务表。如果有任务，则按照任务明细（时间、线上机房ip）向日志服务器请求下载该分片的日志。重放请求到指定的代理服务器。</p>
<p>代理服务端：提供实时回源数据查询服务。并且安装nws缓存服务器等组件，该机器等同于线上机房的软件模块。</p>
<p>实时展示界面：可随时查看实时回源率和一些任务异常状态信息。</p>
<p>图3为客户端和服务端的互动图。图4是任务控制端在任务进行中和其他模块的联动过程。</p>
<p><img src="http://res.cloudinary.com/lumanyu/image/upload/v1578283471/mini_arch_system/ade420cb4068108a80bc08d865ee8bd7_sxidhx.jpg" alt=""></p>
<p>图3 肉鸡和代理服务端的架构</p>
<p><img src="http://res.cloudinary.com/lumanyu/image/upload/v1578283471/mini_arch_system/35ef273cdd78951fe504572d6fd67289_w2yhzs.jpg" alt=""></p>
<p>图4 控制端的任务联动过程</p>
<p><strong>分布式系统特点</strong></p>
<p>日志重放模型核心是一个高性能压测系统，但是需要添加一些逻辑：日志下载、日志分析重构、结果数据收集、数据上报展示。分布式系统核心是：是否做到了可拓展、可恢复、简易搭建、容错、自动化。以下内容会一一展开。</p>
<p>先说说高性能：在一个通用模型中。我们模拟线上日志，这个系统要做到高效、因为我们的重放日志速度要比线上的qps还要快。机器的重放速度决定了分析结果的速度。同时更快的速度，所需要的肉鸡资源更少。笔者在python各个url请求库和golang中，最终敲定使用了golang实现肉鸡。golang做到了和原生c+epoll一样快的速度，但是代码实现容易多了。理论上我们对一台做过代理端性能瓶颈分析。线上日志比模拟日志更复杂，qps适度下降是必然的。Golang这个客户端达到预期目标。</p>
<p>可扩展：在我们可能会随时增加模拟机器集群的肉鸡数量，或者更多的闲置代理服务器资源加入压测任务。所以系统在可用机器数据表随时加入新的机器。</p>
<p><img src="http://res.cloudinary.com/lumanyu/image/upload/v1578283471/mini_arch_system/bc41cc7c73b9682f9b6ddd96101380f3_mxpsgm.jpg" alt=""></p>
<p>图5 系统的动态可扩展</p>
<p>可恢复：分布式系统不同于单机模式。不能避免可能有各种故障，有时候系统部分节点出错了，我们更倾向于不用这个节点，而不是继续使用未处理完成的结果。即非0即1，无中间状态。还有分布式系统网络传输延迟不可控。所以压测系统设计了一套容错机制：包括心跳检测失败，自动在数据表剔除肉鸡服务端。接口异常容错。超时过期未完成任务去除。crontab定时拉取退出进程等。</p>
<p>简易搭建：使用ajs接口，和批处理安装脚本。自动化部署肉鸡和服务端。配置dns解析ip（日志服务器，任务池、回源率结果所在的数据库ip），tcp<br>time_wait状态的复用，千万别忘了还有一些系统限制放开（放开ulimit fd<br>limit，这里设置100000，永久设置需要编辑/etc/security/limits.conf）。如果肉鸡有依赖程序运行库需要同时下载。在肉鸡机器下载肉鸡客户端和配置、在服务端机器下载服务端和配置，下载定时拉起程序脚本，并添加到crontab定时执行。以上都用批处理脚本自动执行。</p>
<p><strong>一些设计范式的思考</strong></p>
<p><strong>Single-productor and Multi-consumer</strong></p>
<p>在肉鸡客户端的设计中：读日志文件一行一条记录，添加到消息管道，然后多个执行worker从消息管道取url，执行模拟请求。消息管道传送的是一条待执行的日志url。IO消耗型程序指的是如果consumer执行访问日志并瞬间完成结果，但是productor需要对日志进行复杂的字符串处理（例如正则之类的），那么它下次取不到数据，就会被管道block住。另外一种是CPU消耗型程序，如果日志url已经预先处理好了，productor只是简单的copy数据给消息管道。而consumer访问url，经过不可预知的网络延迟。那么多个consumer（因为是包括网络访问时间，consumer个数设计超过cpu核数，比如2倍）同时访问，读端速度慢于写端数度。在对一个日志文件进行实验，我们发现处理18w条记录日志的时间是0.3s，而执行完这些url的访问任务则需要3分钟。那么很显然这是一个CPU消耗性进程。如果是IO消耗型的程序。Golang有种叫fan<br>out的消息模型。我们可以这样设计：多个读端去读取多个chan<br>list的chan，一个写端写一个chan。Fanout则将写端的chan，循环写到chan<br>list的chan中。</p>
<p><strong>Map-reduce</strong></p>
<p>我们有时会做一个地理位置一个运营商的机房日志分析。一个机房包含数台机器ip。合理的调度多个肉鸡客户端并行访问日志，可以更快速得到合并回源率数据。</p>
<p>并行机制，经典的map-reduce，日志文件按机房机器ip纬度切片分发任务，启动N个肉鸡同时并行访问，等最后一台肉鸡完成任务时，归并各个肉鸡数据按成功请求数量、成功请求流量、失败请求数量、失败请求流量等方式做统计。同时用于和线上日志做校样。这里的mapper就是肉鸡，产生的数据表，我们按照关注的类型去提取就是reducer。</p>
<p>简化的map-reducer（不基于分布式文件系统），map和reduce中间的数据传递用数据表实现。每个mapper产生的日志数据先放在本地，然后再上报给数据表。但是数据表大小的限制，我们只能上传头部访问url。所以如果用这个办法实现，数据是不完整的，或者不完全正确的数据。因为也许两台肉鸡合并的头部数据正好就包括了某肉鸡未上传的日志（该日志因为没有到达单机肉鸡访问量top的标准）。</p>
<p>那么如何解决这个问题呢，根本原因在于汇总数据所在的文件系统是本地的，不是分布式的（hadoop的hdfs大概就是基于这种需求发明的把）。如果是状态码纬度，这种思路是没问题的，因为http状态码总量就那么少。那么如果是url纬度，比如说某机房给单肉鸡的单次任务在10分钟的url总数据量达到18万条。只看日志重复数&gt;100的肉鸡数据。这样误差最大值是100*肉鸡数，所以对于10台肉鸡的机房，只要是综合合并结果&gt;1000。都是可信任的。如果是域名纬度，少数头部客户流量占比大多数带宽。<br>这也就是所谓的hot-key，少数的hot-key占据了大多数比例的流量。所以域名纬度时，这个时候可以把关注点缩放在指定域名的url列表。如果本地上报给数据表的数据量太大，url也可以考虑进行短地址压缩。当然如果不想弯道超车的话，需要硬解决这个问题，那可能得需要hdfs这种分布式文件系统。</p>
<p><strong>Stream-Processing</strong></p>
<p>我们进行日志客户端系统，需要向日志服务器下载此次任务所需要的日志（一般是一个机器10分钟的访问日志）。首先本地日志会去任务服务器查询重放任务。接着去日志服务器下载。如果该模拟集群是在DC网络组建，那么下载一个10分钟（约150M左右的文件）日志几乎在1两秒内搞定，但是如果这个分布式系统是组建于OC网络，那么OC网络的肉鸡服务器要去DC（考虑机房可靠性，日志服务器架设在DC网络）下载，经过nat转化内网到外网，下载则需要10s左右。如果为了等待日志服务器下载完，也是一笔时间开销。</p>
<p>在分布式系统中，所谓的stream-processing，和batch<br>processing不同的是，数据是无边界的。你不知道什么时候日志下载完。而batch<br>processing的前后流程关系，好比生产流水线的工序，前一道完成，后一道才开始，对于后一道是完全知道前一道的输出结果有多少。</p>
<p>所谓的流式处理则需要在前一道部分输出结果到达时，启动后一道工序，前一道工序继续输出，后一道则需要做出处理事件响应。后一道需要频繁调度程序。</p>
<p>消息系统（message<br>broker）：前一道的部分输出，输入给消息系统。消息系统检测到是完整的一条日志，则可以产生后一道工序的输入。这里我们会碰到一个问题。下载日志的速度（10s）会远远快于执行重放这些日志的速度（3min）。按照一个消息系统可能的动作是：无buffer则丢弃，按照队列缓存住，执行流控同步后一道工序和前一道工序的匹配速度。这里我们选择了按照队列缓存住这个方案。当然在一个严谨的分布式数据库设计，message<br>broker是一个能考率到数据丢失的节点。Broker会把完整数据发给后道工序，同时会把buffer数据缓存到硬盘备份，以防程序core<br>dump。如果对于慢速前道工序，可以进行综合方案配置，丢弃或者流控。这里消息broker不同于数据库，他的中间未处理数据是暂时存储，处理过的消息要清除存储。</p>
<p><strong>总结</strong></p>
<p>当然：现实中的生产线的分布式系统会远比这个复杂，但是本文实现的从0到1的迷你麻雀分布式系统有一定的实践意义。它不是一蹴而就的，不断地版本迭代。当然该系统也完成了作者的kpi-存储模型分析，在中途遇到问题时，进行的设计思考和改良，在此总结分享给大家。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2020/01/06/%E5%BE%AE%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%8C%83%E4%BE%8B/" data-id="ck5chp3bc0001my02ah1hak26" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-我是怎么一步步用go找出压测性能瓶颈" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/05/%E6%88%91%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E6%AD%A5%E6%AD%A5%E7%94%A8go%E6%89%BE%E5%87%BA%E5%8E%8B%E6%B5%8B%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/" class="article-date">
  <time datetime="2020-01-05T08:10:19.000Z" itemprop="datePublished">2020-01-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/05/%E6%88%91%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E6%AD%A5%E6%AD%A5%E7%94%A8go%E6%89%BE%E5%87%BA%E5%8E%8B%E6%B5%8B%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/">我是怎么一步步用go找出压测性能瓶颈</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>序言：</strong></p>
<p>笔者要在线上服务器load日志并且重放来测一些机器性能指标。模拟机器资源比较少，相对的被模拟的线上机器日志量大，假设线上单机qps有1w，那么5台机器组成的集群5w个qps。模拟机器压测客户端需要比5w个qps更快，才有比较意义。</p>
<p><strong>第一章：HTTP初体验</strong></p>
<p>正所谓“人生苦短，我用python”，python自带了urllib2、urllib3以及第三方的request。支持的代理访问、添加请求头基本满足功能需求。笔者用urllib2+multiprocessing库顺利了码完代码运行之，查看qps只有2k多，这显然远远低于需求。在加大进程数到cpu核数的数倍之多，也发现python仅能达到3k多。事出必有因，于是笔者便通过监控界面和shell小工具来找机器各种茬。</p>
<p><strong>第二章：“中世纪黑暗期”</strong></p>
<p>中世纪是黑暗漫长的时期，你做了很多事情，但却输出很少，留下来的是尝试后的经验总结。从cpu、内存、硬盘、网络各方面数据看。cpu使用率90%多，内存用满、硬盘wa很低、网络千兆网卡满载。最首先的是把千兆网卡机器替换成万兆网卡机器。查看timewait的连接数达到1w3多。那就先优化下看起来是”瓶颈”的东西。配置tcp_timestamps=1,<br>tcp_tw_reuse=1, tpc_tw_recycle=1。sysctl<br>-p生效下最新的配置，timewait连接数没下去，并发数没上来。既然硬件该做的设置都完了，那为什么别人家的露娜那么秀，我家的就是一坨屎呢。</p>
<p>再回过头来考虑程序架构问题。反省自己，首先urllib2、request库是网络io阻塞的，其次网络是短连接的，再次这么多进程切换系统开销也很大。在广袤的互联网海洋中遨游了一番，得出的结论就是grequest库可能是个解决办法。gevent是个协程库，它使用greenlet库提供的基于libev实现的高性能异步网络框架。Perfect！看起来是那么的完美。于是又尝试重写了程序。可是性能还是没有上去。那到底是不是python语言自身的限制问题，导致cpu高居不下，并发量又上不去呢？这里留个疑问，到文章的最后再来回答这个问题。</p>
<p><strong>第三章：豁朗开朗</strong></p>
<p>不甘心并且不再纠结于python，用当下网红golang重写下。golang的协程库号称是性能优秀，语言层面支持并行的，易于书写的利器。写完跑一跑，并发量还是上不去。一直保持打死都不放弃的精神，笔者再次用go的第二性能利器自带的golang<br>pprof分析下代码的瓶颈。pprof生成的报告还可以用uber第三方组件go-torch生成更直观的火焰图。如图1所示。从火焰图查看出runtime.gcBgMarkWorker(gc:垃圾回收器)，并且runtime.mallocgc也占用大量cpu时间。接着进行内存占用分析，使用go tool pprof -alloc_space replay1 /tmp/mem.prof查看如图2<br>所示，敲入top10命令，发现pull_worker累加分配了600多G内存，占比93%，list<br>pull_worker命令找到该函数的瓶颈点。这个r4变量的初始化放在一个for循环内，r4是用于临时读取响应body，这个r4每次请求都重复分配，导致内存居高不下，解决办法是把他放在for循环外。</p>
<p><strong>终章：总结</strong></p>
<p>好了，至此单机并发量最高可以到3w了，也差不多达到计划的目标了。用两台这种机器组成的肉鸡就可以满足5w<br>qps的请求了。再来回答之前留下来的问题，python语言并发上不去只是因为，库不支持从外面提供读buffer读取响应body，导致内存暴增，这不是语言本身的问题。相信python并没有那么差。同时，也熟悉了一门新利器go语言。go的原生协程支持和性能分析利器还是非常直观非常好用的，力荐！！</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578212605/golang_performance/14e0bc68dc42a1fed2905112d2aeeb18_xaoo0y.png" alt=""></p>
<p>图1、性能瓶颈前的cpu火焰图</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578212605/golang_performance/af9c0dd95720188d18fa7cf3aee8696a_mrh4uh.png" alt=""></p>
<p>图2、找到内存使用最多的函数</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578212605/golang_performance/900ec2af56a641a4042c7d22fce70404_sun5ys.png" alt=""></p>
<p>图3、找到增长最多的代码</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2020/01/05/%E6%88%91%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E6%AD%A5%E6%AD%A5%E7%94%A8go%E6%89%BE%E5%87%BA%E5%8E%8B%E6%B5%8B%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/" data-id="ck5chp3be0002my02bda19488" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-HTTP2之服务器推送-Server-Push-最佳实践" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/04/HTTP2%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E9%80%81-Server-Push-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/" class="article-date">
  <time datetime="2020-01-04T10:19:08.000Z" itemprop="datePublished">2020-01-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/04/HTTP2%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E9%80%81-Server-Push-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/">HTTP2之服务器推送(Server Push)最佳实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>HTTP/1.X出色地满足互联网的普遍访问需求，但随着互联网的不断发展，其性能越来越成为瓶颈。IETF在2015年发布了HTTP/2标准,<br>着重于提高HTTP的访问体验, HTTP2优势主要包括:<br>二进制传输、头部压缩、多路复用和服务器推送(Server Push)。 截止目前,<br>大部分<em>CDN</em>厂商已经宣布支持HTTP/2，然而”支持”大多省略了服务器推送(ServerPush)特性。估计这和nginx开源版本没有支持Server<br>Push相关。为提供完备的HTTP2能力，腾讯CDN现已完成HTTP/2的Server<br>Push支持，并完成了详细的性能测试。</p>
<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p><strong>在介绍Server<br>Push功能之前，先来分析网站的加载过程。图1是腾讯课堂(<a href="https://ke.qq.com/index.html)的时间瀑布图。" target="_blank" rel="noopener">https://ke.qq.com/index.html)的时间瀑布图。</a></strong></p>
<p>a) 首先浏览器请求主页面index.html，服务端响应内容；</p>
<p>b) 获取到主页应答，浏览器开始解析主页的html标签，发现构建DOM树还需要CSS, GIF,<br>JS等资源；</p>
<p>c) 发起针对CSS,GIF,JS的内容请求；</p>
<p>d) 获取并解析JS和CSS等内容, 然后继续请求依赖资源。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/7b3c9a295994a6130b3573c8bc6e8bc9_gcusik.png" alt=""></p>
<p>图1   腾讯课堂域名的时间瀑布图</p>
<p>图2是简化的浏览器和服务器的交互过程，横轴代表时间轴，每个虚线区间是1个RTT。红色竖线表示DOM 加载完成的时间。从图中可知，虽然存在并发传输, 但主页index.html和依赖的资源common.css、0684a8bf.css、comb.nowrap.0b772fee.js等总体上是顺序的，等待资源响应的时间减慢了主页面加载速度。并发传输并不能提高串行解析的资源访问体验。</p>
<p>如果服务端接收到客户端主请求，能够“预测”主请求的依赖资源，在响应主请求的同时，主动并发推送依赖资源至客户端。客户端解析主请求响应后，可以”无延时”从本地缓存获取依赖资源, 减少访问延时, 提高访问体验，也加大了链路的并发能力。Server<br>Push正是基于此原理来提高网络体验。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/c4e52abacbc71b9ed705072f6642824f_cgpgow.png" alt=""></p>
<p>图3说明了若采用服务端推送的功能，则JS/CSS资源基本可以和HTML资源同步到达，浏览器可以“无延时”获取JS/CSS资源，客户端的延时最多可以减少一个RTT。</p>
<p>构建一个简单的例子来验证我们的说法。图4所示为simple_push.html代码，页面依赖资源simple_push.js和simple_nopush.js, 页面大小均不超过1KB，主要时间消耗在传输延时。如图5所示为推送simple_push.js和不推送simple_nopush.js的效果对比。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/8ddae0d5aa4631a8f729230cab1b0df7_plqf9q.png" alt=""></p>
<p>图4 推送测试HTML代码</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/7bcf5b1677e7706c316be8b1eedc0136_cot90x.png" alt=""></p>
<p>图5 不推送&amp;推送的效果对比</p>
<p>我们上线了一个测试demo网站（<a href="https://http1.gtimg.cn/push/mypush.html）。网页上展示一张世界地图，由400个小图片组成。对比三种访问方式：HTTP/1.1、HTTP/2（无Server" target="_blank" rel="noopener">https://http1.gtimg.cn/push/mypush.html）。网页上展示一张世界地图，由400个小图片组成。对比三种访问方式：HTTP/1.1、HTTP/2（无Server</a><br>Push）和 HTTP/2（Server Push）。Server<br>Push选择推送第150~179个共30个小图。访问性能数据对比如图6所示：可以发现预推送比无推送有一定的性能提升（受网络延时和客户端行为影响，结果存在波动，后文有相应分析）。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/33f3fa07c9aabb85a4823a460a42cea6_xqlt7i.gif" alt=""></p>
<p>图6 demo网站测试</p>
<p>简要介绍了Server<br>Push的优化原理之后，伴随而来的疑问，推送什么资源，怎么去推送，以及比其他优化技术有什么优势？读完本章，这些问题将一一得到解答，文章最后用实例展示Server<br>Push的应用场景和性能优化效果。</p>
<h1 id="一、推送实现"><a href="#一、推送实现" class="headerlink" title="一、推送实现"></a>一、推送实现</h1><h2 id="1、标识依赖资源"><a href="#1、标识依赖资源" class="headerlink" title="1、标识依赖资源"></a>1、标识依赖资源</h2><p>W3C候选推荐标准（<a href="https://www.w3.org/TR/preload/）建议了依赖资源的两种做法：文件内\" target="_blank" rel="noopener">https://www.w3.org/TR/preload/）建议了依赖资源的两种做法：文件内\</a>&lt;link&gt;标签和HTTP头部携带, 表示该资源后续会被使用, 可以预请求, 关键字preload修饰这个资源, 写法如下：</p>
<p><strong>a) 静态Link标签法:</strong></p>
<p>&lt;link rel=”preload” href=”push.css” as=”style”&gt;</p>
<p><strong>b) HTTP头表示法：</strong></p>
<p>Link: &lt;push.css&gt;; rel=preload; as=style</p>
<p>其中rel表明了资源&lt;/push.css&gt;是预加载的，as表明了资源的文件类型。另外，link还可以用nopush修饰，表示浏览器可能已经有该资源缓存，指示有推送能力的服务端不主动推送资源，只有当浏览器先检查到没有缓存，才去指示服务端推送资源，nopush格式写成：</p>
<p>Link: &lt;/app/script.js&gt;; rel=preload; as=script;nopush。</p>
<h2 id="2、推送资源"><a href="#2、推送资源" class="headerlink" title="2、推送资源"></a>2、推送资源</h2><p>用户访问CDN，主要包括直接访问的边缘节点, 若干中间节点和客户源站，路径中的每层都可以对请求做分析，预测可能的依赖资源，通过插入静态&lt;link&gt;标签或者增加响应头部返回给浏览器。 CDN的推送主要采用头部携带推送信息。</p>
<h3 id="a-客户端指定推送资源"><a href="#a-客户端指定推送资源" class="headerlink" title="a) 客户端指定推送资源"></a>a) 客户端指定推送资源</h3><p>客户端通过url或者请求头说明需要的资源url，写法如下：</p>
<p>Url：<a href="http://http2push.gtimg.com/simple_push.html?req-push=simple_push.js" target="_blank" rel="noopener">http://http2push.gtimg.com/simple_push.html?req-push=simple_push.js</a></p>
<p>或者：</p>
<p>GET /simple_push.html HTTP/1.1</p>
<p>Host: http2push.gtimg.com</p>
<p>User-Agent: curl/7.49.1</p>
<p>Accept: */*</p>
<p>X-Push-Url: simple_push.js</p>
<h3 id="b-CDN节点指定推送资源"><a href="#b-CDN节点指定推送资源" class="headerlink" title="b) CDN节点指定推送资源"></a>b) CDN节点指定推送资源</h3><p>CDN节点针对请求资源配置推送资源, 基础配置如下:</p>
<p>location ~ “/simple_push.html$” {</p>
<p>http2_server_push_url /simple_push.js</p>
<p>}</p>
<h3 id="c-源站指定推送资源"><a href="#c-源站指定推送资源" class="headerlink" title="c) 源站指定推送资源"></a>c) 源站指定推送资源</h3><p>通过增加响应头link通知客户端或者CDN节点，后续希望推送的依赖资源，中间具有    推送功能的节点(如CDN节点)可以基于此信息进行资源请求与推送.</p>
<h2 id="3、功能实现"><a href="#3、功能实现" class="headerlink" title="3、功能实现"></a>3、功能实现</h2><p><strong>图7所示为CDN的Server Push架构, 基本流程如下:</strong></p>
<p>a) 用户请求到达服务器之后，依赖资源预测模块根据请求头或者配置预测浏览器需要的资源,该推送资源url必须是和主请求是同一host。如果不属于同一host，服务器拒绝推送资源。</p>
<p>b) 服务器通过PUSH_PROMISE桢告诉浏览器准备推送的资源路径，该信息在原主请求流上发送，必须优先主请求响应发送，否则浏览器可能在推送资源到达前已经发起了依赖资源请求，造成重复和浪费.</p>
<p>c) 依赖资源请求模块构造和主请求一样的请求信息，在本地或后端服务器请求推送资源,并主动创建新的HTTP/2请求流，后续服务器就可以发送资源响应，推送资源响应在服务端创建的流上传输，主页面响应在原始流传输。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/b8392eb26a34ec3a2e4a2ecb0658104f_xqebgv.png" alt=""></p>
<p>图7 CDN的Server Push模块改造示意图</p>
<p><strong>CDN节点的推送资源发送顺序在主请求响应之前，如图8所示，主要基于以下因素考量：</strong></p>
<p>d) 推送资源一般是静态的缓存命中率高的资源，如JS、CSS、字体和图片等。这些资源可以从源站预先推送并缓存到CDN节点。相比之下, 主页面变更较多，需要等待网络IO去源站取数据。同时，CDN边缘节点到浏览器的RTT一般是比CDN节点到源站的RTT更短。所以在取到主页面最新响应之前，有充足的时间去推送资源。</p>
<p>e) 资源推送可以探测提高TCP拥塞窗口，窗口逐渐增大，后续可以一次性发送完主页面响应。TCP拥塞窗口对推送影响将在下文第三部分讨论。</p>
<p>f) 在等待主请求响应的网络IO时间期间，推送资源可以是无优先级关系，资源推送优先级对推送影响也将在下文第三部分讨论。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/99bc12ffe2a587b76f1a557d58ee545b_ok3eo2.png" alt=""></p>
<p>图8 推送时间点位于主页面响应之前</p>
<h1 id="二Server-Push技术对比"><a href="#二Server-Push技术对比" class="headerlink" title="二Server Push技术对比"></a>二Server Push技术对比</h1><h2 id="1、纵向对比"><a href="#1、纵向对比" class="headerlink" title="1、纵向对比"></a>1、纵向对比</h2><p><strong>Server Push相对应没有Server Push的具体提升如下：</strong></p>
<p>a) Nopush加载耗时：Tnopush = RTT+ max(RTT,<br>size(HTML)/BandWidth)+size(JS)/BandWidth</p>
<p>b) push耗时：Tpush = RTT + size(HTML)/BandWidth + size(JS)/BW</p>
<p>c) 改善效率：diff =1 - Tpush/TnoPush</p>
<p>所以决定推送是否有改善性能的衡量因素是size(HTML/BandWidth)和RTT谁大。这里引入BDP(BandWidth-Delay<br>product, 带宽时延乘积)概念。BDP描述了单位时间内该带宽能传输的数据大小。如果size(HTML)&lt;BDP，推荐使用push；反之不推荐使用push。</p>
<h2 id="2、横向对比"><a href="#2、横向对比" class="headerlink" title="2、横向对比"></a>2、横向对比</h2><p>HTTP/1.1中有个资源内联（Resource<br>Inlining）技术，把资源内容拷贝到HTML标签中。比如说&lt;script&gt;可以装载js的内容，&lt;style&gt;可以装载CSS的内容等。这样JS或者CSS的内容就会在第一个响应中推送给浏览器。虽然说它可以做到网站加速。但是它有很多server<br>push没有的缺点。例如资源不能脱离HTML被浏览器单独缓存，并且这个资源在多个url中重复传输多遍。这在多个url共享这个资源的场景是不明智的做法。而使用Server<br>Push，在CDN能适用更丰富的应用场景。</p>
<h1 id="三、使用场景分析"><a href="#三、使用场景分析" class="headerlink" title="三、使用场景分析"></a>三、使用场景分析</h1><p>理论上，在带宽足够的环境下，把需要的资源预先推送给客户端，必然能够节省获取资源时间，提升页面访问速度。但由于TCP慢启动、资源加载优先级、浏览器缓存等因素约束，我们在实际测试中发现，Server<br>Push并不总能带来页面加载性能的提升。本节深入探讨下什么场景下的资源适合使用推送。</p>
<h2 id="1、TCP慢启动"><a href="#1、TCP慢启动" class="headerlink" title="1、TCP慢启动"></a>1、TCP慢启动</h2><p>先复习下TCP慢启动特性：为了防止网络拥塞，TCP将放弃超出拥塞窗口大小的数据。只有当拥塞串口大小的数据传输完成，这个窗口大小将乘以2。如此，能够传输的数据以2的倍数增长。假设拥塞窗口大小为14kB，下图展示了某些情况下，推送比不推送的效率没有提升。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/d4147519309b95d0c8a63a4338fab127_ahah1r.png" alt=""></p>
<p>图9 tcp慢启动对服务器推送的影响</p>
<p>对比图9中子图1和子图2，子图1虽然预推送了/style.css，但是第一次RTT只传输了/style.css的4KB数据，剩下的16KB在第2个RTT完成。子图1总共需要2个RTT的时间，和子图2没有进行推送用了同样多的时间。子图3使用了3RTT完成了整个网站的传输，这会比没有推送使用更多的时间。</p>
<h2 id="2、资源加载优先级"><a href="#2、资源加载优先级" class="headerlink" title="2、资源加载优先级"></a>2、资源加载优先级</h2><p>先看下面一个网站例子：</p>
<p>&lt;html&gt;</p>
<p>&lt;head&gt;</p>
<p>&lt;script src=”1.js”&gt;&lt;/script&gt;</p>
<p>&lt;script src=”3.js”&gt;&lt;/scirpt&gt;</p>
<p>&lt;script src=”4.js”&gt;&lt;/script&gt;</p>
<p>&lt;/head&gt;</p>
<p>&lt;body&gt;&lt;/body&gt;</p>
<p>&lt;/html&gt;</p>
<p>其中1.js会调用2.js文件，3.js和4.js没有调用其他JS。</p>
<p>正常没推送的例子加载时间表格会是</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/4bf1d97393d3f42603adb6276e3c3378_vuoeib.png" alt=""></p>
<p>图10  资源加载优先级的nopush&amp;push效果图</p>
<p>可以看出是因为1.js的加载优先级本应该在3.js和4.js之前，但是预先推送了3.js和4.js，然而1.js需要重新请求，并触发2.js请求，导致等待1RTT接收2.js。所以Push比No<br>Push的效率更差。</p>
<h2 id="3、内核缓冲区"><a href="#3、内核缓冲区" class="headerlink" title="3、内核缓冲区"></a>3、内核缓冲区</h2><p>HTTP/2的请求优先级并不能影响已经在内核发送缓冲区的数据。假设内核发送缓冲区大小比TCP拥塞串口大，导致服务端发送低优先级的数据，存在内核缓冲区。这时，后续有高优先级的响应必须等内核缓冲区空出才能被完成。假设我们访问一个HTML页面，这个HTML页面需要回源站取数据，而HTML需要的静态JS资源缓存在CDN边缘节点上。在回源站的等待时间内，把静态JS资源发送给浏览器。如果这时候静态JS资源很大，塞满了内核发送缓冲区，此时HTML响应已经到达CDN边缘节点，却不得不等内核缓冲区有空间才能继续发送。等待浏览器解析HTML内容后续的link请求也会被推迟。</p>
<h2 id="4、浏览器缓存"><a href="#4、浏览器缓存" class="headerlink" title="4、浏览器缓存"></a>4、浏览器缓存</h2><p>推送浏览器已缓存的资源有可能使的加载时间更长，并且浪费带宽资源。重复推送已缓存的资源，如果没有额外的空闲带宽传输，网络会阻塞它之后正常的请求，导致拖累了整个网站的加载时间。</p>
<h1 id="四、网站测试"><a href="#四、网站测试" class="headerlink" title="四、网站测试"></a>四、网站测试</h1><p>我们对现网一些网页进行Server<br>Push性能测试，因为推送要求同一个域名下的HTTP/2请求，为了规避非HTTP/2和跨余名带来的干扰，我们设置了代理节点，代理节点完成HTTP/2支持和域名收归，同时配置Server<br>Push功能，观察网页的加载收益。为了准确测试Push带来网络时延变化，需要稳定的网络环境，在chrome设置网络环境mytest（RTT:<br>200ms, Download: 29Mb/s, Upload: 14Mb/s），以下的例子都在该网络环境进行测试。</p>
<h2 id="1、腾讯新闻"><a href="#1、腾讯新闻" class="headerlink" title="1、腾讯新闻"></a>1、腾讯新闻</h2><p>按照前面描述的推送适用场景，用这个腾讯新闻页面（<a href="https://news.qq.com/a/20171031/032143.htm）做测试。主请求页面大小为11.6K。可以看出，预先推送js、css、图片等资源给客户端带来的网站性能变快。" target="_blank" rel="noopener">https://news.qq.com/a/20171031/032143.htm）做测试。主请求页面大小为11.6K。可以看出，预先推送js、css、图片等资源给客户端带来的网站性能变快。</a></p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/eea91812eeb923624a997b7f6858dea7_xs4rv8.png" alt=""></p>
<p>图11  腾讯新闻页面</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/202101ce9b9f825594160516b0fdfc33_kfwunn.png" alt=""></p>
<p>图12  腾讯新闻页面的无推送&amp;推送对比图</p>
<h2 id="2、腾讯客服"><a href="#2、腾讯客服" class="headerlink" title="2、腾讯客服"></a>2、腾讯客服</h2><p>腾讯客服页面不支持HTTPS协议。之所以用这个页面是因为该网页页面主请求比较小，并且有JS、CSS触发的次优先级资源请求。我们把这个网页下载下来，并做了一些推送资源域名收归等必要的处理，放在CDN边缘节点做测试。这并没有改变网站的资源和请求顺序，不影响测试效果。</p>
<p>图13是腾讯客服的页面。图14列出腾讯客服页面的所有请求。我们关注下具体几种情况的时间轴：无推送、推送小文件、推送大文件。小文件推送预先在第一个RTT把3个第3层请求才能触发的资源（tcss.ping.js、cdn_djl.js、layer.css）预先推送给浏览器。大文件推送是预先推送了indexBanner.png。</p>
<p>从图14中的无推送和推送3个小文件的子图中，红色虚竖线是指不包括indexBanner.png的加载完成时间，由于3个小文件（尤其是次优先级请求tcss.ping.js）的提取推送，比无推送的时间延迟要短。但是又从无推送和推送大文件的子图中看到，如果无优先级顺序地推送大文件indexBanner.png（782KB）对缩短网站时延无帮助。</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/1a6495a72d74409bfc8c4ed1c4fa1ba1_hn1d42.png" alt=""></p>
<p>图13  腾讯客服页面</p>
<p><img src="https://res.cloudinary.com/lumanyu/image/upload/v1578188924/http2_server_push/6d9c40fb3ad771ffe79460bc6837f19b_snmtsa.png" alt=""></p>
<p>图14  无推送&amp;推送小文件&amp;推送大文件的对比图</p>
<h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>虽然本章的测试用例只是庞大互联网网页的冰山一角，文章不能覆盖各种网页场景。但是以下的一些总结建议是有实践意义的。</p>
<p>1、在合适的时机，推送合适的资源，Push比No<br>Push带来的网站时延提升是明显的。在网络带宽足够承载推送资源的前提下，我们预先推送浏览器后续请求需要的资源，网站的整体加载时间得到缩短。但是现实网络环境有不一样的延时和带宽。慢速网络环境影响TCP拥塞窗口增长的速度，除非主页面请求足够小，Push才能看到效果。</p>
<p>2、即使是错误地实施某些推送策略（比如说推送过大文件），带来的最严重后果，也就是改善不明显。所以建议是多做一些推送策略的尝试，直到把合适的资源在合适的时机把资源推送给浏览器。</p>
<p>3、网站往HTTP/2的环境迁移是个趋势。迁往HTTP/2需要将页面的所有请求尽量收归到同一域名，并且剥离出主页面的资源文件成多个独立的请求。假如你的网站已迁移到HTTP/2，而且网站的主请求不大，但是可能会触发很多资源请求。建议push这些资源。另外不要推送存放在浏览器cookie的资源，这只会浪费带宽。</p>
<p>4、目前的Server<br>Push推送机制没有解决浏览器已经具有资源缓存，而服务器已经推送到网络中，虽然浏览器可以发送RST桢拒绝推送流，但是服务器推送的资源已经在网络中等待浏览器接收。现在已经有一些规范草案（<a href="https://tools.ietf.org/html/draft-kazuho-h2-cache-digest-01）尝试用协商缓存摘要来解决问题。" target="_blank" rel="noopener">https://tools.ietf.org/html/draft-kazuho-h2-cache-digest-01）尝试用协商缓存摘要来解决问题。</a></p>
<p>5、CDN中的<a href="https://cloud.tencent.com/product/clb?from=10909" target="_blank" rel="noopener">负载均衡</a>机制可能会将低优先级的推送资源送入到系统缓存区，这会影响高优先级资源的推送效率问题。引入QUIC替代TCP，可以对缓存中推送资源进行分级，高优先级资源先发。</p>
<p>6、未来或将引入AI分析取代固定推送实现智能化推送。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2020/01/04/HTTP2%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E9%80%81-Server-Push-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/" data-id="ck5chp3bh0003my0258137sdz" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-socket接口api的深度探究" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/07/socket%E6%8E%A5%E5%8F%A3api%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%8E%A2%E7%A9%B6/" class="article-date">
  <time datetime="2019-08-07T07:28:09.000Z" itemprop="datePublished">2019-08-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/07/socket%E6%8E%A5%E5%8F%A3api%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%8E%A2%E7%A9%B6/">socket接口api的深度探究</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、socket接口使用"><a href="#一、socket接口使用" class="headerlink" title="一、socket接口使用"></a>一、socket接口使用</h1><h2 id="1-1-socket抽象层"><a href="#1-1-socket抽象层" class="headerlink" title="1.1 socket抽象层"></a>1.1 socket抽象层</h2><p>Linux内核net/socket.c定义了一套socket的操作api。图1展示了socket层所处与TCP/IP协议栈之上和应用层之下。</p>
<p><img src="https://ask.qcloudimg.com/draft/1198598/ltkdnt65lr.png?imageView2/2/w/1620" alt="img">图1 socket层的位置</p>
<h2 id="1-2-一些需要预先知道的内核操作api"><a href="#1-2-一些需要预先知道的内核操作api" class="headerlink" title="1.2 一些需要预先知道的内核操作api"></a>1.2 一些需要预先知道的内核操作api</h2><p>socket层大量使用了这些内核操作api，完成协议栈的调用入口。在深度探究socket层实现之前，先来了解下这些内核api。</p>
<ul>
<li>fget_light()和fput_light()：轻量级的文件查找入口。多任务对同一个文件进行操作，所以需要对文件做引用计数。fget_light在当前进程的struct files_struct中根据所谓的用户空间文件描述符fd来获取文件描述符。另外，根据当前fs_struct是否被多各进程共享来判断是否需要对文件描述符进行加锁，并将加锁结果存到一个int中返回， fput_light则根据该结果来判断是否需要对文件描述符解锁。fget_light()/fput_light是fget/fput的变形，不用考虑多进程共享同一个文件表而导致的竞争避免锁。fget/fput是指在文件表的引用计数+1/-1</li>
<li>sockfd_lookup_light根据fd找到相应的socket object（内核真正操作的对象）。</li>
<li>so_xxx: 内核相关socket操作接口。socket object操作协议栈的api入口。</li>
<li>in_pcballoc()。分配内核内存，内存名字叫Internet protocol control block。</li>
<li>in_pcbbind(), 绑定IN_PCB到指定的地址，如果不指定地址，那么会寻找一个可用的端口进行绑</li>
<li>in_pcblookup()： 指定的端口是否可用。</li>
<li>sbappend()追加数据到发送缓冲区。</li>
<li>so-&gt;so_proto-&gt;<em>pr_usrreq 是socket  object操作协议栈的函数</em></li>
<li>tcp_ursreq()是tcp    协议栈操作的入口函数，支持以下操作类型：PRU_ATTACH，PRU_BIND，PRU_LISTEN, PRU_ACCEPT, PRU_CONNECT, PRU_SHUTDOWN，PRU_ABORT， PRU_DETACH，PRU_SEND，PRU_SENDOOB，PRU_RCVD,PRU_RCVOOB</li>
<li>tcp_newtcpcb()。TCP control block被分配，socket描述符指向的正是这个TCP control block。</li>
<li>tcp_attach().</li>
<li>tcp_xxx: tcp_close(), tcp_disconect(),tcp_drop()</li>
<li>pr_xxx: 一套socket层和协议栈通信的接口，包括pr_usrreq()，pr_input()，pr_output()，pr_ctlinput()，pr_ctloutput()。</li>
</ul>
<h2 id="1-3-socket函数api"><a href="#1-3-socket函数api" class="headerlink" title="1.3 socket函数api"></a>1.3 socket函数api</h2><h3 id="1-3-1-socket函数"><a href="#1-3-1-socket函数" class="headerlink" title="1.3.1 socket函数"></a>1.3.1 socket函数</h3><ul>
<li><strong>功能：</strong>在内核创建一个socket对象，并返回引用的操作fd。通过这个fd操作这个socket</li>
<li><strong>实现：</strong>通过in_pcballoc() 分配Internet control block的内存。接着调用tcp_newtcpcb()分配TCP control block的内存。初始化TCP定时器等。链接Internet control block的内存和TCP control block的内存</li>
<li><strong>注意：</strong>此时TCP为CLOSED状态</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/1xsxjbqbj7.png?imageView2/2/w/1620" alt="img">图2 创建socket函数的流程</p>
<h3 id="1-3-2-bind函数"><a href="#1-3-2-bind函数" class="headerlink" title="1.3.2 bind函数"></a>1.3.2 bind函数</h3><ul>
<li><strong>功能：</strong>指派一个ip地址给socket，并且分配一个fd返回</li>
<li><strong>实现：</strong>检查端口没有被重复绑定过。网卡有带ip。如果没有指定端口，则会先尝试从reserverd port(&lt;1024)开始找，如果还没有可用会继续从ephemeral port （比如1024~5000）找。</li>
<li><strong>注意：</strong>客户端不需要显示bind，因为connect在内部已经自动实现了bind的逻辑。</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/gatrbevlaf.png?imageView2/2/w/1620" alt="img">图3 bind的流程</p>
<h3 id="1-3-3-connect函数"><a href="#1-3-3-connect函数" class="headerlink" title="1.3.3 connect函数"></a>1.3.3 connect函数</h3><ul>
<li><strong>功能：</strong>通过该函数建立和对端的有状态连接。connect成功调用意味着TCP三次握手成功</li>
<li><strong>实现：</strong>在用户态校验服务端地址（要connect的对端地址）是合法的，移入到内核空间，进入内核态进行connect操作。检查是否以及bind，如果没有bind，进行bind。接着soisconnecting会置tcp状态为SYN_SENT，调用tcp_output会发送SYN包。进入睡眠直到协议栈唤醒，如果成功置ESTABLISHED。</li>
<li><strong>注意：</strong>connect之前的显示调用bind不是必须的，当然有也可以。</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/apq2ohw62h.png?imageView2/2/w/1620" alt="img">图4 connect流程</p>
<h3 id="1-3-4-listen函数"><a href="#1-3-4-listen函数" class="headerlink" title="1.3.4 listen函数"></a>1.3.4 listen函数</h3><ul>
<li>功能：listen表示协议栈可以接收新的连接请求，同时正常处理连接中的请求数量是有限的（backlog=521）。</li>
<li>实现：迁移TCP状态从CLOSED迁移到LISTEN</li>
<li>注意：</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/xidviiv9p2.png?imageView2/2/w/1620" alt="img">图5 listen流程</p>
<h3 id="1-3-5-accept流程"><a href="#1-3-5-accept流程" class="headerlink" title="1.3.5 accept流程"></a>1.3.5 accept流程</h3><ul>
<li><strong>功能：</strong>从listen socket创建新的通信socket</li>
<li><strong>实现：</strong>默认accept是阻塞，直到accept到连接，创建新的socket，唤醒客户端，返回这个socket，并且把外网ip和端口从内核协议栈拷贝给用户态应用层。</li>
<li><strong>注意：</strong>除了accpet，还有accept4（为什么叫4，因为有4个形参）比accept多了一个参数，可以传flag到系统调用。可以看到两者的区别仅仅在于accept4()有第四个参数flags，这个参数如果为0，就跟accept()一样；下面的两个参数可以用按位OR来获取不同的行为。SOCK_NONBLOCK:为新打开的文件描述符设置O_NONBLOCK标志位，如果是accept需要和fcntl()搭配使用，这样设置的效果和accept4是一样的，区别就是用accept的话需要多调用个fcntl函数。SOCK_CLOEXEC: 为新打开的文件描述符设置FD_CLOEXEC标志位，该标志位的作用是在进程使用fork()加上execve()的时候自动关闭打开的文件描述符。其实使用fcntl()设置FD_CLOEXEC标志位（也就是用open()的时候设置的O_CLOEXEC标志位）也能达到同样的效果，但跟fcntl()有什么不同呢？在多线程环境中，如果使用fcntl()会多出一步操作，这样就可能形成竞争。而使用accept4()就可以直接在打开的文件描述符上设置，可以消除竞争的问题。（原则上该竞争在那些新建文件描述符的调用中都存在，所以很多linux的系统调用都做了类似的处理）</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/n89um71tqk.png?imageView2/2/w/1620" alt="img">图6 accept流程</p>
<h3 id="1-3-6-send-write函数"><a href="#1-3-6-send-write函数" class="headerlink" title="1.3.6 send/write函数"></a><strong>1.3.6 send/write函数</strong></h3><ul>
<li><strong>功能：</strong>发送数据</li>
<li><strong>实现：</strong>验证socket和connection状态，分配空间，拷贝消息到内核</li>
<li><strong>注意：</strong>发送函数有4个api：sendto，sendmsg，write，writev。send只能操作网络fd，而write更通用，可用处理任意通用fd。另外send允许您为实际操作指定某些选项。读/写是“通用”文件描述符函数，而recv / send稍微更专门化(例如，您可以设置一个标志忽略SIGPIPE，或者发送带外消息…)。</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/qv21rkh195.png?imageView2/2/w/1620" alt="img">图7 send流程</p>
<h3 id="1-3-7-recv-read函数"><a href="#1-3-7-recv-read函数" class="headerlink" title="1.3.7 recv/read函数"></a>1.3.7 recv/read函数</h3><ul>
<li>功能：接收数据</li>
<li>实现：除了拷贝内核接收区的数据到应用层，还发送窗口更新信息给网络对端</li>
<li>注意：recv和send一样也提供了4套接口：recvfrom，recvmsg，read，readv。recv和read的区别如同send和write的区别</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/h7fx6clcb5.png?imageView2/2/w/1620" alt="img">图8 recv流程</p>
<h3 id="1-3-8-close函数"><a href="#1-3-8-close函数" class="headerlink" title="1.3.8 close函数"></a><strong>1.3.8 close函数</strong></h3><ul>
<li><strong>功能：关闭连接</strong></li>
<li><strong>实现：如果是</strong>listening socket：遍历两个保持正在连接的pending conection的队列。阻止进一步的accept导致的tcp连接下一步状态迁移。比如说连接处于SYN_RCVD，那么会发送RST包。如果是其他socket，那么会置detach 连接在socket的control block。并且检查linger选项和linger time。如果linger time为0，那么会立即drop掉这个tcp连接。否则则有可能会发送FIN包关闭连接</li>
<li>注意：</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/jwz94ngzec.png?imageView2/2/w/1620" alt="img">图9 close流程</p>
<h3 id="1-3-9-shundown函数"><a href="#1-3-9-shundown函数" class="headerlink" title="1.3.9 shundown函数"></a><strong>1.3.9 shundown函数</strong></h3><p>拒绝新的网络读数据，释放资源，丢弃读缓冲区，并且关闭读端连接，协议栈将写端缓冲区buff发送出去，并且关闭写端。写端关闭有可能会发送FIN包。</p>
<h1 id="二、深入理解过程"><a href="#二、深入理解过程" class="headerlink" title="二、深入理解过程"></a>二、深入理解过程</h1><h2 id="2-1-tcp的三次握手"><a href="#2-1-tcp的三次握手" class="headerlink" title="2.1 tcp的三次握手"></a>2.1 tcp的三次握手</h2><p><img src="https://ask.qcloudimg.com/draft/1198598/2rm1q619bt.png?imageView2/2/w/1620" alt="img">图10 tcp三次连接（上半部分）</p>
<p><img src="https://ask.qcloudimg.com/draft/1198598/oqt0dy00im.png?imageView2/2/w/1620" alt="img">图11 tcp连接的下半部分</p>
<h2 id="2-2-为什么是3次，而不是2次"><a href="#2-2-为什么是3次，而不是2次" class="headerlink" title="2.2 为什么是3次，而不是2次"></a>2.2 为什么是3次，而不是2次</h2><p>此时已经客户端已经显示ESTABLISHED，是否可代表只需要两次握手。</p>
<p>为什么不是两次握手，而是三次握手，会有什么问题</p>
<p>还有一种失效连接的处理，客户端向服务端发出的SYN包延迟了，服务端没收到，客户端再重新发一个SYN包，然后服务端新建了这个连接。那么此时如果之前由于网络节点的延迟又达到了B，那么B会以为是A发起的新连接。于是B同意，并向A发起确认，但是此时A根本不会理会。B一直等A发送应用层数据，但是A并没有这个连接的发送任务。</p>
<h1 id="三、异常情况"><a href="#三、异常情况" class="headerlink" title="三、异常情况"></a>三、异常情况</h1><h2 id="3-1-accept过程的异常"><a href="#3-1-accept过程的异常" class="headerlink" title="3.1  accept过程的异常"></a>3.1  accept过程的异常</h2><h3 id="3-1-1-SYN没成功的重试次数"><a href="#3-1-1-SYN没成功的重试次数" class="headerlink" title="3.1.1 SYN没成功的重试次数"></a>3.1.1 SYN没成功的重试次数</h3><p>服务端会根据/proc/sys/net/ipv4/tcp_synack_retries（我的机器设置为5）设置的重试次数，重发SYN+ACK，</p>
<h3 id="3-1-2-backlog已满的状态怎么办"><a href="#3-1-2-backlog已满的状态怎么办" class="headerlink" title="3.1.2 backlog已满的状态怎么办"></a>3.1.2 backlog已满的状态怎么办</h3><ul>
<li>服务端发送SYN+ACK，客户端收到后会回复ACK，如果此时ACCEPT队列仍处于已满状态，退避2^n后再次重试，直到超过重试次数超过/proc/sys/net/ipv4/tcp_synack_retries设置的次数，服务端链接状态SYNC_RECV-&gt;CLOSED，客户端链接状态为ESTABLISHED。</li>
<li>如果内核参数/proc/sys/net/ipv4/tcp_abort_on_overflow 是0，服务端会忽略最后一个ACK，此时服务端的TCP链接处于SYN_RECV半连接状态，客户端的TCP链接处于ESTABLISHED状态，客户端以为链接创建成功，服务端却处于半连接状态，<strong>状态不一致！其实这种不一致在TCP/IP协议里经常出现，处理方式一般都是重试和退避。</strong></li>
<li>如果内核参数/proc/sys/net/ipv4/tcp_abort_on_overflow 是1，则服务端会回复一个RST包，由SYN_RECV-&gt;CLOSED，客户端链接 ESTABLISHED-&gt;CLOSED。</li>
</ul>
<h2 id="3-2-send过程中"><a href="#3-2-send过程中" class="headerlink" title="3.2 send过程中"></a>3.2 send过程中</h2><h3 id="3-2-1-进程退出"><a href="#3-2-1-进程退出" class="headerlink" title="3.2.1 进程退出"></a>3.2.1 进程退出</h3><p>先用kill -9方法，其实kill -9不能模拟服务器断电的情况。进程退出总共有8中情况：</p>
<p>有8种方式使进程终止，其中前5种为正常终止，它们是</p>
<ul>
<li>从 main 返回</li>
<li>调用 exit</li>
<li>调用 _exit 或 _Exit</li>
<li>最后一个线程从其启动例程返回</li>
<li>最后一个线程调用 pthread_exit</li>
</ul>
<p>　　异常终止有3种，它们是</p>
<ul>
<li>调用 abort</li>
<li>接到一个信号并终止</li>
<li>最后一个线程对取消请求做出响应</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/eofremt2zg.png?imageView2/2/w/1620" alt="img">图12 进程正常退出</p>
<p>通过tcp抓包发现，有正常的四次挥手过程</p>
<h3 id="3-2-2-拔电源、拔网线、交换机瘫痪的办法"><a href="#3-2-2-拔电源、拔网线、交换机瘫痪的办法" class="headerlink" title="3.2.2 拔电源、拔网线、交换机瘫痪的办法"></a>3.2.2 拔电源、拔网线、交换机瘫痪的办法</h3><p> 那如果进程是由于服务器断电，导致的失连，服务器会怎么样。</p>
<p>操作步骤如下：设备B监听端口。设备A通过connect设备B的监听端口。设备A的进程睡眠，此时断掉设备B的网卡。拔网卡的命令是</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link <span class="keyword">set</span> eth0 down; </span><br><span class="line">ip link <span class="keyword">set</span> eth0 up;</span><br></pre></td></tr></table></figure>

<p>设备A停止睡觉，send数据，返回值正是这个数据的长度，如果在继续send，会返回成功吗，会接受到对方RST包吗，为什么。这里看到进程发送完退出，会进入一段次数的退避重传（15次，共924秒，哪里配置的），然后没有FIN挥手过程。</p>
<p>send为什么成功的解释是，send只会探测到本地的错误，而不会探测到网络错误。</p>
<p>重试次数的配置：</p>
<ul>
<li><p>／proc/sys/net/ipv4／tcp_retries1</p>
<p> 这个值影响由于某些错误引起的没有ACK的RTO重传和上报这些错误给网路层的时间。</p>
</li>
</ul>
<p>​    RFC 1122推荐至少3次重传，这是个默认值。</p>
<ul>
<li><p>／proc/sys/net/ipv4/tcp_retries2</p>
<p> 这个值影响当RTO重传仍没收到ACK的TCP连接的超时时间。</p>
<p>给定一个值N，假定一个TCP连接带有TCP_RTO_MIN的初始RTO的指数值会重传N次，在第(N+1)个RTO时杀死这个连接。默认值是15，生成一个假想的超时时间是924.6秒，和一个有效超时的下限。当超过这个假设的超时时间，TCP会在第一个RTO就会超时.RFC1122推荐至少超时时间有100秒，相当于这个值等于8.</p>
</li>
</ul>
<p><img src="https://ask.qcloudimg.com/draft/1198598/hvogscqgt5.png?imageView2/2/w/1620" alt="img">图13 断电掉线，send成功</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mariolu.xyz/2019/08/07/socket%E6%8E%A5%E5%8F%A3api%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%8E%A2%E7%A9%B6/" data-id="ck5chp3b60000my028l7m9med" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/13/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2020/01/13/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BF%E9%97%AEunix-socket%E6%96%87%E4%BB%B6/">如何通过命令行访问unix socket文件</a>
          </li>
        
          <li>
            <a href="/2020/01/13/%E6%83%8A%E7%BE%A4%E6%95%88%E5%BA%94/">惊群效应</a>
          </li>
        
          <li>
            <a href="/2020/01/06/%E5%BE%AE%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E8%8C%83%E4%BE%8B/">微型分布式架构设计范例</a>
          </li>
        
          <li>
            <a href="/2020/01/05/%E6%88%91%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E6%AD%A5%E6%AD%A5%E7%94%A8go%E6%89%BE%E5%87%BA%E5%8E%8B%E6%B5%8B%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/">我是怎么一步步用go找出压测性能瓶颈</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Mario Lu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>